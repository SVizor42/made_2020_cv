{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бейзлайн для распознавания автомобильных номеров. \n",
    "\n",
    "MADE with love :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision import transforms\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сейчас будет много вспомогательных функций, которые можно промотать\n",
    "\n",
    "Чтобы было наглядно и не приходилось лезть в модули, чтобы посмотреть, какая функция что делает, оставил для наглядности пока что все в ноутбуке. Можно аккуратно перенести в модули :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бинарный поиск для приближения предсказанной маски 4-хугольником\n",
    "def simplify_contour(contour, n_corners=4):\n",
    "    n_iter, max_iter = 0, 1000\n",
    "    lb, ub = 0., 1.\n",
    "\n",
    "    while True:\n",
    "        n_iter += 1\n",
    "        if n_iter > max_iter:\n",
    "            print('simplify_contour didnt coverege')\n",
    "            return None\n",
    "\n",
    "        k = (lb + ub)/2.\n",
    "        eps = k*cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, eps, True)\n",
    "\n",
    "        if len(approx) > n_corners:\n",
    "            lb = (lb + ub)/2.\n",
    "        elif len(approx) < n_corners:\n",
    "            ub = (lb + ub)/2.\n",
    "        else:\n",
    "            return approx\n",
    "\n",
    "# Отображаем 4-хугольник в прямоугольник \n",
    "# Спасибо ulebok за идею \n",
    "# И вот этим ребятам за реализацию: https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\n",
    "def four_point_transform(image, pts):\n",
    "    \n",
    "    rect = order_points(pts)\n",
    "    \n",
    "    tl, tr, br, bl = pts\n",
    "    \n",
    "    width_1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    width_2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    max_width = max(int(width_1), int(width_2))\n",
    "    \n",
    "    height_1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    height_2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    max_height = max(int(height_1), int(height_2))\n",
    "    \n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [max_width, 0],\n",
    "        [max_width, max_height],\n",
    "        [0, max_height]], dtype = \"float32\")\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (max_width, max_height))\n",
    "    return warped\n",
    "\n",
    "def order_points(pts):\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    \n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    \n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    \n",
    "    return rect\n",
    "\n",
    "\n",
    "# Визуализируем детекцию (4 точки, bounding box и приближенный по маске контур)\n",
    "def visualize_prediction_plate(file, model, device='cuda', verbose=True, thresh=0.0, \n",
    "                               n_colors=None, id_to_name=None):\n",
    "    img = Image.open(file)\n",
    "    img_tensor = my_transforms(img)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model([img_tensor.to(device)])\n",
    "    prediction = predictions[0]\n",
    "    \n",
    "    if n_colors is None:\n",
    "        n_colors = model.roi_heads.box_predictor.cls_score.out_features\n",
    "    \n",
    "    palette = sns.color_palette(None, n_colors)\n",
    "    \n",
    "    img = cv2.imread(file, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    image = img\n",
    "    \n",
    "    blackImg = np.zeros(image.shape, image.dtype)\n",
    "    blackImg[:,:] = (0, 0, 0)\n",
    "    for i in range(len(prediction['boxes'])):\n",
    "        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n",
    "        label = int(prediction['labels'][i].cpu())\n",
    "        score = float(prediction['scores'][i].cpu())\n",
    "        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n",
    "        name = id_to_name[label]\n",
    "        color = palette[label]\n",
    "        \n",
    "        if verbose:\n",
    "            if score > thresh:\n",
    "                print ('Class: {}, Confidence: {}'.format(name, score))\n",
    "        if score > thresh:            \n",
    "            crop_img = image[y_min:y_max, x_min:x_max]\n",
    "            print('Bounding box:')\n",
    "            show_image(crop_img, figsize=(10, 2))\n",
    "            \n",
    "            # В разных версиях opencv этот метод возвращает разное число параметров\n",
    "            # contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n",
    "            #_,contours,_ = cv2.findContours((mask > 0.05).astype(np.uint8), 1, 1)\n",
    "            contours,_ = cv2.findContours((mask > 0.05).astype(np.uint8), 1, 1)\n",
    "            approx = simplify_contour(contours[0], n_corners=4)\n",
    "            \n",
    "            if approx is None:\n",
    "                x0, y0 = x_min, y_min\n",
    "                x1, y1 = x_max, y_min\n",
    "                x2, y2 = x_min, y_max\n",
    "                x3, y3 = x_max, y_max\n",
    "#                 points = [[x_min, y_min], [x_min, y_max], [x_max, y_min],[x_max, y_max]]\n",
    "            else:\n",
    "                x0, y0 = approx[0][0][0], approx[0][0][1]\n",
    "                x1, y1 = approx[1][0][0], approx[1][0][1]\n",
    "                x2, y2 = approx[2][0][0], approx[2][0][1]\n",
    "                x3, y3 = approx[3][0][0], approx[3][0][1]\n",
    "                \n",
    "            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n",
    "            \n",
    "            \n",
    "            points = np.array(points)\n",
    "            crop_mask_img = four_point_transform(img, points)\n",
    "            print('Rotated img:')\n",
    "            crop_mask_img = cv2.resize(crop_mask_img, (320, 64), interpolation=cv2.INTER_AREA)\n",
    "            show_image(crop_mask_img, figsize=(10, 2))\n",
    "            if approx is not None:\n",
    "                cv2.drawContours(image, [approx], 0, (255,0,255), 3)\n",
    "            image = cv2.circle(image, (x0, y0), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            image = cv2.circle(image, (x1, y1), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            image = cv2.circle(image, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            image = cv2.circle(image, (x3, y3), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            \n",
    "            image = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), np.array(color) * 255, 2)\n",
    "            \n",
    "    show_image(image)\n",
    "    return prediction\n",
    "\n",
    "# Просто показать картинку. С семинара\n",
    "def show_image(image, figsize=(16, 9), reverse=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    if reverse:\n",
    "        plt.imshow(image[...,::-1])\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Переводит предсказания модели в текст. С семинара\n",
    "def decode(pred, alphabet):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], alphabet))\n",
    "    return outputs\n",
    "\n",
    "def pred_to_string(pred, alphabet):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([alphabet[c] for c in out])\n",
    "    return out\n",
    "    \n",
    "\n",
    "        \n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "# Чтобы без проблем сериализовывать json. Без него есть нюансы\n",
    "class npEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/made-cv-hw2/data'\n",
    "TRAIN_SIZE = 0.9\n",
    "BATCH_SIZE = 4\n",
    "BATCH_SIZE_OCR = 64\n",
    "DETECTOR_MODEL_PATH = '../input/cv-hw2-models/detector_kaggle_b4_all.pt'\n",
    "OCR_MODEL_PATH = 'ocr_kaggle_b4_all.pt'\n",
    "\n",
    "all_marks = load_json(os.path.join(DATA_PATH, 'train_cleaned.json'))\n",
    "test_start = int(TRAIN_SIZE * len(all_marks))\n",
    "train_marks = all_marks[:test_start]\n",
    "train_detection_marks = all_marks[:test_start]\n",
    "val_marks = all_marks[test_start:]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Находим номера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Модель для детекции\n",
    "\n",
    "В задании есть данные о 4 точках, которые задают номер. Эти 4 точки - почти всегда не прямоугольник, а произвольный четырехугольник. Будем предсказывать:\n",
    "\n",
    "- bounding box, который окружает точки (детекция)\n",
    "- маску, заполненную тем, что внутри 4-х точек (сегментация)\n",
    "\n",
    "Поэтому, возьмем maskrcnn. Будем обучать несколько последних солев. Этого с запасом хватает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_model():\n",
    "    \n",
    "    model = models.detection.maskrcnn_resnet50_fpn(\n",
    "        pretrained=True, \n",
    "        pretrained_backbone=True,\n",
    "        progress=True, \n",
    "        num_classes=91, \n",
    "    )\n",
    "\n",
    "    num_classes = 2\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.roi_heads.box_predictor = box_predictor\n",
    "    \n",
    "    mask_predictor = MaskRCNNPredictor(256, 256, num_classes)\n",
    "    model.roi_heads.mask_predictor = mask_predictor\n",
    "\n",
    "    # Заморозим все слои кроме последних\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    for param in model.backbone.fpn.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Датасет для детекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionDataset(Dataset):\n",
    "    def __init__(self, marks, img_folder, transforms=None):\n",
    "        \n",
    "        self.marks = marks\n",
    "        self.img_folder = img_folder\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.marks[idx]\n",
    "        img_path = f'{self.img_folder}{item[\"file\"]}'\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        w, h = img.size\n",
    "        \n",
    "        box_coords = item['nums']\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        for box in box_coords:\n",
    "            points = np.array(box['box'])  \n",
    "            x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n",
    "            x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n",
    "            boxes.append([x0, y0, x2, y2])\n",
    "            labels.append(1)\n",
    "            \n",
    "            # Здесь мы наши 4 точки превращаем в маску\n",
    "            # Это нужно, чтобы кроме bounding box предсказывать и, соответственно, маску :)\n",
    "            nx, ny = w, h\n",
    "            poly_verts = points\n",
    "            x, y = np.meshgrid(np.arange(nx), np.arange(ny))\n",
    "            x, y = x.flatten(), y.flatten()\n",
    "            points = np.vstack((x,y)).T\n",
    "            path = Path(poly_verts)\n",
    "            grid = path.contains_points(points)\n",
    "            grid = grid.reshape((ny,nx)).astype(int)\n",
    "            masks.append(grid)\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        labels = torch.as_tensor(labels)\n",
    "        masks = torch.as_tensor(masks)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "        }\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.marks)\n",
    "    \n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = DetectionDataset(\n",
    "    marks=train_detection_marks, \n",
    "    img_folder='../input/made-cv-hw2/data/', \n",
    "    transforms=my_transforms\n",
    ")\n",
    "#val_dataset = DetectionDataset(\n",
    "#    marks=val_marks, \n",
    "#    img_folder='../input/made-cv-hw2/data/', \n",
    "#    transforms=my_transforms\n",
    "#)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn, \n",
    ")\n",
    "\n",
    "#val_loader = DataLoader(\n",
    "#    val_dataset, \n",
    "#    batch_size=BATCH_SIZE, \n",
    "#    drop_last=False,\n",
    "#    num_workers=4,\n",
    "#    collate_fn=collate_fn, \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Обучаем модель для детекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = get_detector_model()\n",
    "model.load_state_dict(torch.load(DETECTOR_MODEL_PATH))\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидироваться на чем-то нет смысла, ибо лосс перестает падать еще до того момента, как пройдет 1-я эпоха. Т.е. лосс на трейне вполне валидный, ибо модель видит данные в первый раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5, verbose=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "\n",
    "    print_loss = []\n",
    "    for i, (images, targets) in tqdm.tqdm(enumerate(train_loader), leave=False, position=0, total=len(train_loader)):\n",
    "\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print_loss.append(losses.item())\n",
    "        if (i + 1) % 20 == 0:\n",
    "            mean_loss = np.mean(print_loss)\n",
    "            print(f'Loss: {mean_loss:.7f}')\n",
    "            scheduler.step(mean_loss)\n",
    "            print_loss = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DETECTOR_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(DATA_PATH, 'test/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction_plate(np.random.choice(test_images), model, id_to_name={1: 'plate'}, thresh=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Предсказываем bounding box-ы и маску. \n",
    "\n",
    "- Маску превращаем в 4-угольный полигон. Сохраняем предсказания в json\n",
    "- Если маска не приближается 4-угольником (редко такое бывает, бинарный поиск по гиперпараметру не работает), то просто записываем координаты bounding box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_SCORE = 0.97\n",
    "TRESHOLD_MASK = 0.05\n",
    "\n",
    "preds = []\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for file in tqdm.tqdm(test_images, position=0, leave=False):\n",
    "\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    img_tensor = my_transforms(img)\n",
    "    with torch.no_grad():\n",
    "        predictions = model([img_tensor.to(device)])\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    pred = dict()\n",
    "    pred['file'] = file\n",
    "    pred['nums'] = []\n",
    "\n",
    "    for i in range(len(prediction['boxes'])):\n",
    "        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n",
    "        label = int(prediction['labels'][i].cpu())\n",
    "        score = float(prediction['scores'][i].cpu())\n",
    "        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n",
    "\n",
    "        if score > THRESHOLD_SCORE:            \n",
    "            # В разных версиях opencv этот метод возвращает разное число параметров\n",
    "            contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n",
    "            #_,contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n",
    "            approx = simplify_contour(contours[0], n_corners=4)\n",
    "            \n",
    "            if approx is None:\n",
    "                x0, y0 = x_min, y_min\n",
    "                x1, y1 = x_max, y_min\n",
    "                x2, y2 = x_min, y_max\n",
    "                x3, y3 = x_max, y_max\n",
    "            else:\n",
    "                x0, y0 = approx[0][0][0], approx[0][0][1]\n",
    "                x1, y1 = approx[1][0][0], approx[1][0][1]\n",
    "                x2, y2 = approx[2][0][0], approx[2][0][1]\n",
    "                x3, y3 = approx[3][0][0], approx[3][0][1]\n",
    "                \n",
    "            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n",
    "\n",
    "            pred['nums'].append({\n",
    "                'box': points,\n",
    "                'bbox': [x_min, y_min, x_max, y_max],\n",
    "            })\n",
    "\n",
    "    preds.append(pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', 'w') as json_file:\n",
    "    json.dump(preds, json_file, cls=npEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Распознаем номера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Датасет для распознавания номеров\n",
    "\n",
    "Из особенностей - на каждый номер мы генерируем bounding box + вырезаем по точкам и превращаем в прямоугольник наш 4-угольник по данным точкам. Т.е. 2 картинки на номер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from PIL import ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, marks, img_folder, alphabet, transforms=None):\n",
    "        ocr_marks = []\n",
    "        for items in marks:\n",
    "            file_path = items['file']\n",
    "            for box in items['nums']:\n",
    "                \n",
    "                ocr_marks.append({\n",
    "                    'file': file_path,\n",
    "                    'box': np.clip(box['box'], 0, None).tolist(),\n",
    "                    'text': box['text'],\n",
    "                    'boxed': False,\n",
    "                })\n",
    "                            \n",
    "                # Добавим точки, запакованные в BoundingBox. \n",
    "                # Вместо аугментации rotate. Датасет будет в 2 раза больше\n",
    "                \n",
    "                #Клипаем, ибо есть отрицательные координаты\n",
    "                points = np.clip(box['box'], 0, None) \n",
    "                x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n",
    "                x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n",
    "\n",
    "                ocr_marks.append({\n",
    "                    'file': file_path,\n",
    "                    'box': [x0, y0, x2, y2],\n",
    "                    'text': box['text'],\n",
    "                    'boxed': True,\n",
    "                })\n",
    "                \n",
    "        self.marks = ocr_marks\n",
    "        self.img_folder = img_folder\n",
    "        self.transforms = transforms           \n",
    "        self.alphabet = alphabet\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.marks[idx]\n",
    "        img_path = os.path.join(self.img_folder, item[\"file\"])\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if item['boxed']:\n",
    "            x_min, y_min, x_max, y_max = item['box']\n",
    "            img = img[y_min:y_max, x_min:x_max]\n",
    "        else:\n",
    "            points = np.clip(np.array(item['box']), 0, None)\n",
    "            img = four_point_transform(img, points)\n",
    "            \n",
    "        text = item['text']\n",
    "        seq = [self.alphabet.find(char) + 1 for char in text]\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        output = {\n",
    "            'img': img,\n",
    "            'text': text,\n",
    "            'seq': seq,\n",
    "            'seq_len': seq_len\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.marks)\n",
    "    \n",
    "    \n",
    "class Resize(object):\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        w_from, h_from = img.shape[1], img.shape[0]\n",
    "        w_to, h_to = self.size\n",
    "        \n",
    "        # Сделаем разную интерполяцию при увеличении и уменьшении\n",
    "        # Если увеличиваем картинку, меняем интерполяцию\n",
    "        interpolation = cv2.INTER_AREA\n",
    "        if w_to > w_from:\n",
    "            interpolation = cv2.INTER_CUBIC\n",
    "        \n",
    "        img = cv2.resize(img, dsize=self.size, interpolation=interpolation)\n",
    "        return img\n",
    "    \n",
    "class RandomBlur(object):\n",
    "    def __init__(self, blur=5):\n",
    "        self.blur = blur\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        b = np.random.randint(- self.blur, self.blur + 1)\n",
    "        if b > 0:\n",
    "            img = cv2.blur(img, (b, b))\n",
    "\n",
    "        return img\n",
    "    \n",
    "def gaussian_blur(img):\n",
    "    image = np.array(img)\n",
    "    image_blur = cv2.GaussianBlur(image,(65, 65),10)\n",
    "    new_image = image_blur\n",
    "    return new_image\n",
    "    \n",
    "ocr_train_transforms = transforms.Compose([\n",
    "    Resize(size=(320, 64)),\n",
    "    RandomBlur(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ocr_val_transforms = transforms.Compose([\n",
    "    Resize(size=(320, 64)),\n",
    "    RandomBlur(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_vocab_from_marks(marks):\n",
    "    train_texts = []\n",
    "    for item in marks:\n",
    "        for num in item['nums']:\n",
    "            train_texts.append(num['text'])\n",
    "\n",
    "    counts = Counter(''.join(train_texts))\n",
    "    alphabet = ''.join(set(''.join(train_texts)))\n",
    "    corted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    char_to_idx = {item[0]: idx + 1 for idx, item in enumerate(corted_counts)}\n",
    "    idx_to_char = {idx:char for char, idx in char_to_idx.items()}\n",
    "    return char_to_idx, idx_to_char, alphabet\n",
    "\n",
    "char_to_idx, idx_to_char, alphabet = get_vocab_from_marks(train_marks)\n",
    "\n",
    "train_ocr_dataset = OCRDataset(\n",
    "    marks=train_marks, \n",
    "    img_folder=DATA_PATH, \n",
    "    alphabet=alphabet,\n",
    "    transforms=ocr_train_transforms,\n",
    ")\n",
    "val_ocr_dataset = OCRDataset(\n",
    "    marks=val_marks, \n",
    "    img_folder=DATA_PATH, \n",
    "    alphabet=alphabet,\n",
    "    transforms=ocr_val_transforms\n",
    ")\n",
    "\n",
    "def collate_fn_ocr(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    Accepts list of dataset __get_item__ return values (dicts).\n",
    "    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for sample in batch:\n",
    "        images.append(sample[\"img\"])\n",
    "        seqs.extend(sample[\"seq\"])\n",
    "        seq_lens.append(sample[\"seq_len\"])\n",
    "        texts.append(sample[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch\n",
    "\n",
    "train_ocr_loader = DataLoader(\n",
    "    train_ocr_dataset, \n",
    "    batch_size=BATCH_SIZE_OCR, \n",
    "    drop_last=True,\n",
    "    num_workers=0, # Почему-то у меня виснет DataLoader, если запустить несколько потоков\n",
    "    collate_fn=collate_fn_ocr,\n",
    "    timeout=0,\n",
    "    shuffle=True # Чтобы повернутые дубли картинок не шли подряд\n",
    ")\n",
    "\n",
    "val_ocr_loader = DataLoader(\n",
    "    val_ocr_dataset, \n",
    "    batch_size=BATCH_SIZE_OCR, \n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_ocr, \n",
    "    timeout=0,\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Модель для распознования текста номера\n",
    "\n",
    "Взял RCNN из семинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=(64, 320), output_len=20):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, 'resnet18')(pretrained=True)\n",
    "#        resnet = getattr(models, 'resnet34')(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(kernel_size=(h // 32, 1))        \n",
    "        self.proj = nn.Conv2d(w // 32, output_len, kernel_size=1)\n",
    "  \n",
    "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
    "    \n",
    "    def apply_projection(self, x):\n",
    "        \"\"\"Use convolution to increase width of a features.\n",
    "        Accepts tensor of features (shaped B x C x H x W).\n",
    "        Returns new tensor of features (shaped B x C x H x W').\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.proj(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        return x\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Apply conv layers\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Pool to make height == 1\n",
    "        features = self.pool(features)\n",
    "        \n",
    "        # Apply projection to increase width\n",
    "        features = self.apply_projection(features)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class SequencePredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.rnn = nn.GRU(input_size=input_size,\n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout,\n",
    "                       bidirectional=bidirectional)\n",
    "        \n",
    "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
    "        self.fc = nn.Linear(in_features=fc_in,\n",
    "                         out_features=num_classes)\n",
    "    \n",
    "    def _init_hidden_(self, batch_size):\n",
    "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
    "        Accepts batch size.\n",
    "        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
    "        \"\"\"\n",
    "        num_directions = 2 if self.rnn.bidirectional else 1\n",
    "        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n",
    "        \n",
    "    def _prepare_features_(self, x):\n",
    "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
    "        Accepts tensor x shaped (B x (C=1) x H x W).\n",
    "        Returns new tensor shaped (W x B x H).\n",
    "        \"\"\"\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._prepare_features_(x)\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        h_0 = self._init_hidden_(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        x, h = self.rnn(x, h_0)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class CRNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        alphabet=alphabet,\n",
    "        cnn_input_size=(64, 320), \n",
    "        cnn_output_len=20,\n",
    "        rnn_hidden_size=128, \n",
    "        rnn_num_layers=2, \n",
    "        rnn_dropout=0.3, \n",
    "        rnn_bidirectional=False\n",
    "    ):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        \n",
    "        self.features_extractor = FeatureExtractor(\n",
    "            input_size=cnn_input_size, \n",
    "            output_len=cnn_output_len\n",
    "        )\n",
    "        \n",
    "        self.sequence_predictor = SequencePredictor(\n",
    "            input_size=self.features_extractor.num_output_features,\n",
    "            hidden_size=rnn_hidden_size, \n",
    "            num_layers=rnn_num_layers,\n",
    "            num_classes=(len(alphabet) + 1), \n",
    "            dropout=rnn_dropout,\n",
    "            bidirectional=rnn_bidirectional\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features_extractor(x)\n",
    "        sequence = self.sequence_predictor(features)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Обучаем модель для распознавания текста номера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настроечные параметры для OCR\n",
    "EPOCHS_OCR = 1\n",
    "BIDIRECTIONAL = False\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN(rnn_hidden_size=HIDDEN_SIZE, rnn_bidirectional=BIDIRECTIONAL)\n",
    "#crnn.load_state_dict(torch.load(OCR_MODEL_PATH))\n",
    "crnn.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_val = -1\n",
    "best_val_loss = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with validation\n",
    "for epoch in range(EPOCHS_OCR):\n",
    "    crnn.train()\n",
    "\n",
    "    print_loss = []\n",
    "    train_losses = []   \n",
    "    for i, batch in enumerate(tqdm.tqdm(train_ocr_loader, total=len(train_ocr_loader), leave=False, position=0, desc=\"training...\")):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        seqs_gt = batch[\"seq\"]\n",
    "        seq_lens_gt = batch[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "        log_probs = F.log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "        loss = F.ctc_loss(\n",
    "            log_probs=log_probs,  # (T, N, C)\n",
    "            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "            input_lengths=seq_lens_pred,  # N\n",
    "            target_lengths=seq_lens_gt # N\n",
    "        )  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print_loss.append(loss.item())\n",
    "        if (i + 1) % 20 == 0:\n",
    "            mean_loss = np.mean(print_loss)\n",
    "            print(f' Loss: {np.mean(print_loss):.7f}')\n",
    "            scheduler.step(mean_loss)\n",
    "            print_loss = [] \n",
    "    \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "#    print('Epoch #', epoch, ': train_loss =', train_loss)\n",
    "\n",
    "    crnn.eval()\n",
    "\n",
    "    print_loss = []\n",
    "    val_losses = []\n",
    "    for i, b in enumerate(tqdm.tqdm(val_ocr_loader, total=len(val_ocr_loader), desc=\"validation...\")):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seqs_pred = crnn(images).cpu()\n",
    "        log_probs = F.log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "        loss = F.ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                        targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                        input_lengths=seq_lens_pred,  # N\n",
    "                        target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        print_loss.append(loss.item())\n",
    "        if (i + 1) % 20 == 0:\n",
    " #           mean_loss = np.mean(print_loss)\n",
    "            print(f' Loss: {np.mean(print_loss):.7f}')\n",
    " #           scheduler.step(mean_loss)\n",
    "            print_loss = [] \n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)   \n",
    "#    scheduler.step(val_loss)\n",
    "    print('\\nEpoch #', epoch, f': train_loss = {train_loss:.7f}, val_loss = {val_loss:.7f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print('Saving best model on epoch #', epoch)\n",
    "        torch.save(crnn.state_dict(), 'ocr-last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(crnn.state_dict(), OCR_MODEL_PATH)\n",
    "crnn.load_state_dict(torch.load('ocr-last.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Наконец, делаем предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_marks = load_json('test.json')\n",
    "crnn.eval()\n",
    "resizer = Resize()\n",
    "\n",
    "file_name_result = [] \n",
    "plates_string_result = []\n",
    "\n",
    "for item in tqdm.tqdm(test_marks, leave=False, position=0):\n",
    "\n",
    "    img_path = item[\"file\"]\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    results_to_sort = []\n",
    "    for box in item['nums']:\n",
    "        x_min, y_min, x_max, y_max = box['bbox']\n",
    "        img_bbox = resizer(img[y_min:y_max, x_min:x_max])\n",
    "        img_bbox = my_transforms(img_bbox)\n",
    "        img_bbox = img_bbox.unsqueeze(0)\n",
    "\n",
    "\n",
    "        points = np.clip(np.array(box['box']), 0, None)\n",
    "        img_polygon = resizer(four_point_transform(img, points))\n",
    "        img_polygon = my_transforms(img_polygon)\n",
    "        img_polygon = img_polygon.unsqueeze(0)\n",
    "\n",
    "        preds_bbox = crnn(img_bbox.to(device)).cpu().detach()\n",
    "        preds_poly = crnn(img_polygon.to(device)).cpu().detach()\n",
    "\n",
    "        preds = preds_poly + preds_bbox\n",
    "        num_text = decode(preds, alphabet)[0]\n",
    "\n",
    "        results_to_sort.append((x_min, num_text))\n",
    "\n",
    "    results = sorted(results_to_sort, key=lambda x: x[0])\n",
    "    num_list = [x[1] for x in results]\n",
    "\n",
    "    plates_string = ' '.join(num_list)\n",
    "    file_name = img_path[img_path.find('test/'):]\n",
    "\n",
    "    file_name_result.append(file_name)\n",
    "    plates_string_result.append(plates_string)\n",
    "    \n",
    "df_submit = pd.DataFrame({'file_name': file_name_result, 'plates_string': plates_string_result})\n",
    "df_submit.to_csv('subm_last2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то так :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.sort_values(by='file_name', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
